# Терминальный чат с Ollama

Терминальное чат-приложение на Kotlin для работы с локальной моделью Ollama через OpenAI-совместимый API.

## Требования

- Java 21 или выше
- Запущенный локальный сервер Ollama
- Установленная модель (по умолчанию `llama3`)

## Установка и запуск Ollama

1. Установите Ollama: https://ollama.ai/
2. Запустите Ollama сервер: `ollama serve`
3. Установите модель: `ollama pull llama3`

## Запуск приложения

```bash
./gradlew run
```

## Конфигурация

Приложение поддерживает настройку через переменные окружения:

- `OLLAMA_BASE_URL` - базовый URL Ollama API (по умолчанию: `http://localhost:11434/v1`)
- `OLLAMA_MODEL` - имя модели (по умолчанию: `llama3`)
- `OLLAMA_API_KEY` - API ключ (по умолчанию: `ollama`, используется только для совместимости)

Пример запуска с кастомными настройками:

```bash
OLLAMA_MODEL=mistral OLLAMA_BASE_URL=http://localhost:11434/v1 ./gradlew run
```

## Использование

1. Запустите приложение
2. Введите ваше сообщение после приглашения `User> `
3. Получите ответ от модели
4. Для выхода введите `exit` или `quit`

## Пример диалога

```
=== Терминальный чат с локальной моделью Ollama ===
Модель: llama3
Введите 'exit' или 'quit' для выхода.

User> Привет! Как дела?

ИИ:
Привет! У меня всё отлично, спасибо! Я готов помочь тебе с любыми вопросами. Как дела у тебя?

User> exit
Завершение работы. Пока!
```

## Функции

- ✅ Интерактивный чат в терминале
- ✅ Поддержка истории диалога в рамках сессии
- ✅ Работа с любыми моделями Ollama
- ✅ Настройка через переменные окружения
- ✅ Корректное завершение работы по командам `exit`/`quit`
- ✅ Обработка ошибок подключения

## Архитектура

- `Main.kt` - точка входа и основной цикл приложения
- `OllamaChatClient.kt` - клиент для работы с Ollama API
- Использует нативный HTTP клиент Java для запросов к Ollama
- JSON сериализация через kotlinx.serialization
